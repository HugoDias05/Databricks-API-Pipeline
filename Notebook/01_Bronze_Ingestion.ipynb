{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b6762c-f565-4eb9-ab15-1278be6e50d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 01_Bronze_Ingestion.py - Extracts data from the API and saves to the Bronze Layer\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n",
    "from pyspark.sql.types import StringType\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Persistence Configuration (ADJUST IF NEEDED)\n",
    "CATALOG_NAME = \"workspace\" \n",
    "SCHEMA_NAME = \"default\"\n",
    "BRONZE_TABLE_NAME = \"omdb_releases_bronze\"\n",
    "FULL_BRONZE_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{BRONZE_TABLE_NAME}\"\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = dbutils.secrets.get(scope=\"omdb_scope\", key=\"api_key\") \n",
    "BASE_URL = \"http://www.omdbapi.com/\"\n",
    "search_term = \"Star Wars\" \n",
    "MAX_PAGES = 3             \n",
    "current_page = 1\n",
    "all_records = []\n",
    "\n",
    "print(f\"Starting API search for: {search_term}\")\n",
    "\n",
    "# --- 1. EXTRACTION (PYTHON) ---\n",
    "# Loop through pages to collect all search results\n",
    "while current_page <= MAX_PAGES:\n",
    "    params = {\"apikey\": API_KEY, \"s\": search_term, \"page\": current_page}\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"Response\") == \"True\":\n",
    "            all_records.extend(data.get(\"Search\", []))\n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "    except RequestException as e:\n",
    "        print(f\" - Error during API request: {e}\")\n",
    "        break\n",
    "\n",
    "# 01_Bronze_Ingestion.py - Extracts data from the API and saves to the Bronze Layer\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n",
    "from pyspark.sql.types import StringType\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Persistence Configuration (ADJUST IF NEEDED)\n",
    "CATALOG_NAME = \"workspace\" \n",
    "SCHEMA_NAME = \"default\"\n",
    "BRONZE_TABLE_NAME = \"omdb_releases_bronze\"\n",
    "FULL_BRONZE_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{BRONZE_TABLE_NAME}\"\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = dbutils.secrets.get(scope=\"omdb_scope\", key=\"api_key\") \n",
    "BASE_URL = \"http://www.omdbapi.com/\"\n",
    "search_term = \"Star Wars\" \n",
    "MAX_PAGES = 3             \n",
    "current_page = 1\n",
    "all_records = []\n",
    "\n",
    "print(f\"Starting API search for: {search_term}\")\n",
    "\n",
    "# --- 1. EXTRACTION (PYTHON) ---\n",
    "# Loop through pages to collect all search results\n",
    "while current_page <= MAX_PAGES:\n",
    "    params = {\"apikey\": API_KEY, \"s\": search_term, \"page\": current_page}\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"Response\") == \"True\":\n",
    "            all_records.extend(data.get(\"Search\", []))\n",
    "            current_page += 1\n",
    "        else:\n",
    "            # Stops the loop if API response is 'False' or no more data\n",
    "            break\n",
    "    except RequestException as e:\n",
    "        print(f\" - Error during API request: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n--- 2. LOAD TO BRONZE LAYER ---\")\n",
    "\n",
    "# Check if the list is empty BEFORE trying to create the DataFrame\n",
    "if not all_records:\n",
    "    # This logs the error instead of using dbutils.notebook.exit(), which can cause syntax issues\n",
    "    print(\"API returned no data. Exiting pipeline without writing data.\")\n",
    "else:\n",
    "    # Convert Python List to Spark DataFrame\n",
    "    df_bronze = spark.createDataFrame(all_records)\n",
    "\n",
    "    # Add audit metadata and ensure raw data is treated as StringType\n",
    "    df_bronze = df_bronze.select(*(col(c).cast(StringType()) for c in df_bronze.columns)) \\\n",
    "                         .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "                         .withColumn(\"search_query\", lit(search_term))\n",
    "\n",
    "    # Persist as Delta Table (Overwrite the raw data on each run)\n",
    "    df_bronze.write \\\n",
    "             .format(\"delta\") \\\n",
    "             .mode(\"overwrite\") \\\n",
    "             .saveAsTable(FULL_BRONZE_PATH)\n",
    "\n",
    "    print(f\"Bronze Table '{FULL_BRONZE_PATH}' created with {df_bronze.count()} records.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
